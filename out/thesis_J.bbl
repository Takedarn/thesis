\begin{thebibliography}{10}

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp. 4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{hinton2015distillingknowledgeneuralnetwork}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network, 2015.

\bibitem{jiao-etal-2020-tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
\newblock {T}iny{BERT}: Distilling {BERT} for natural language understanding.
\newblock In Trevor Cohn, Yulan He, and Yang Liu, editors, {\em Findings of the Association for Computational Linguistics: EMNLP 2020}, pp. 4163--4174, Online, November 2020. Association for Computational Linguistics.

\bibitem{rai2025compressedmodelstrustequivalentlarge}
Rohit~Raj Rai, Chirag Kothari, Siddhesh Shelke, and Amit Awekar.
\newblock Compressed models are not trust-equivalent to their large counterparts, 2025.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{sundararajan2017axiomaticattributiondeepnetworks}
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
\newblock Axiomatic attribution for deep networks, 2017.

\bibitem{vaswani2023attentionneed}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2023.

\bibitem{wang-etal-2018-glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural language understanding.
\newblock In Tal Linzen, Grzegorz Chrupa{\l}a, and Afra Alishahi, editors, {\em Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}}, pp. 353--355, Brussels, Belgium, November 2018. Association for Computational Linguistics.

\bibitem{wang2024jaccardmetriclossesoptimizing}
Zifu Wang, Xuefei Ning, and Matthew~B. Blaschko.
\newblock Jaccard metric losses: Optimizing the jaccard index with soft labels, 2024.

\bibitem{makino2024idealnumberofstepforintegratedgradients}
{牧野雅紘, 浅妻佑弥, 佐々木翔大, 鈴木潤}.
\newblock Integrated gradients における理想の積分ステップ数はインスタンス毎に異なる.
\newblock  言語処理学会第30回年次大会, 2024.

\end{thebibliography}
