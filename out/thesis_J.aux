\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {第1章}はじめに}{1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}背景}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}目的}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}本論文の構成}{2}{}\protected@file@percent }
\citation{hinton2015distillingknowledgeneuralnetwork}
\citation{devlin2019bertpretrainingdeepbidirectional}
\citation{jiao-etal-2020-tinybert}
\citation{sundararajan2017axiomaticattributiondeepnetworks}
\citation{wang-etal-2018-glue}
\@writefile{toc}{\contentsline {chapter}{\numberline {第2章}関連研究}{3}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}知識蒸留}{3}{}\protected@file@percent }
\newlabel{kd-fundamental}{{2.1}{3}{}{section.2.1}{}}
\citation{devlin-etal-2019-bert}
\citation{vaswani2023attentionneed}
\newlabel{eq:ce}{{2.1}{4}{}{equation.2.1}{}}
\newlabel{eq:kd}{{2.2}{4}{}{equation.2.2}{}}
\newlabel{eq:total}{{2.3}{4}{}{equation.2.3}{}}
\citation{jiao-etal-2020-tinybert}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}事前学習済み言語モデル}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}BERTの事前学習}{5}{}\protected@file@percent }
\newlabel{bert-model}{{2.2.1}{5}{}{subsection.2.2.1}{}}
\newlabel{eq:mlm}{{2.4}{5}{}{equation.2.4}{}}
\newlabel{eq:nsp}{{2.5}{5}{}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}教師モデルとしてのBERT}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}知識蒸留によって構築されたモデル}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}TinyBERT}{5}{}\protected@file@percent }
\newlabel{tinybert-model}{{2.3.1}{5}{}{subsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{データ拡張 (data augmentation)}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{学習の流れ}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{事前学習段階 (pre-training)}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{下流タスクのファインチューニング (downstream fine-tuning)}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}モデルの推論根拠の解釈に関する研究}{6}{}\protected@file@percent }
\newlabel{model-explain}{{2.4}{6}{}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Integrated Gradient}{6}{}\protected@file@percent }
\newlabel{integrated-gradient}{{2.4.1}{6}{}{subsection.2.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}本研究の特徴}{7}{}\protected@file@percent }
\newlabel{methodlogy-characteristic}{{2.5}{7}{}{section.2.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第3章}提案手法}{8}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces 図のキャプション}}{8}{}\protected@file@percent }
\newlabel{fig1}{{3.1}{8}{}{figure.3.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第4章}実験・評価}{9}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces 表のキャプション}}{9}{}\protected@file@percent }
\newlabel{table1}{{4.1}{9}{}{table.4.1}{}}
\bibstyle{jplain}
\bibdata{reference}
\@writefile{toc}{\contentsline {chapter}{\numberline {第5章}おわりに}{10}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{devlin2019bertpretrainingdeepbidirectional}{1}
\bibcite{devlin-etal-2019-bert}{2}
\bibcite{hinton2015distillingknowledgeneuralnetwork}{3}
\bibcite{jiao-etal-2020-tinybert}{4}
\bibcite{sundararajan2017axiomaticattributiondeepnetworks}{5}
\bibcite{vaswani2023attentionneed}{6}
\bibcite{wang-etal-2018-glue}{7}
\gdef \@abspage@last{17}
