\documentclass[12pt,a4j]{jreport}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\begin{document}
	\thispagestyle{empty}
\begin{center}
修士論文\\% どちらか一方を消してください．
\vfill
推論における重要度単語を転移させる知識蒸留法\\\
\vfill
武田 遥暉\\
\vfill
主指導教員  白井 清昭\\
\vfill
北陸先端科学技術大学院大学\\
先端科学技術研究科\\
（情報科学）\\ %取得希望学位
\vfill
令和8年3月\\ % 学位授与年月
\vfill
\end{center}
\newpage
\pagestyle{empty}
\centerline{Abstract}
English abstract (1200 words)\\
〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇
\clearpage
\centerline{概要}
日本語の概要\\
〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇〇

\clearpage\setcounter{page}{0}\pagenumbering{roman}\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables
\clearpage\setcounter{page}{0}\pagenumbering{arabic}

\chapter{はじめに}

\section{背景}
大規模言語モデル（Large Language Model; LLM）は、大規模テキストで事前学習された言語モデルであり、文脈情報を高次元表現として獲得することでさまざまな言語タスクにおいて高い性能を示している。特に分類モデルとして用いる場合、入力文全体の意味情報を統合した表現を利用することで、文書分類や感情分析などにおいて高い性能を発揮する。

このような能力を実現するため近年のLLMは大規模なモデルサイズを有しており、その運用には多数のGPUをはじめとする計算機資源を必要とする。その結果、十分な資金力を持つ一部の組織のみが利用可能である点や、クラウド経由でGPU資源にアクセスできない環境では導入が困難である点が、実社会への幅広い展開を妨げる課題となっている。

そのため、LLMを教師モデル、より少ないパラメータを持つモデルを生徒モデルとし、教師モデルの性能を可能な限り維持しながら、タスク遂行能力を生徒モデルへ転移する手法として知識蒸留が提案されている。特に分類タスクにおいては、性能をほぼ維持したままモデルサイズを半分程度まで削減可能であるなど、高い有効性が示されており知識蒸留はモデル軽量化の代表的な手法として確立している。

しかしながら、知識蒸留は、計算資源の制約下において教師モデルの代替として利用可能な生徒モデルを構築することを目的とした技術であるにもかかわらず、蒸留によって学習された生徒モデルは分類精度を維持できている一方、推論時に重要と判断する単語が教師モデルと一致しないことが報告されている。このような差異は、医療分野や金融分野など判断根拠の信頼性が重視される領域において致命的な問題となり得るため、生徒モデルを教師モデルの単純な代替として用いることには依然として課題が残っている。

\section{目的}
本研究では、教師モデルと同じ単語に注目して生徒モデルが推論を行うよう学習させることを目的とする。対象タスクに対して十分に学習された教師モデルを用意し、以下の手順に従って知識蒸留を適用する。

まず、教師モデルよりも小規模な未学習の生徒モデルを準備し、両モデルに同一の入力インスタンスを与えて推論を行う。次に、得られた推論結果に対してIntegrated Gradientsを用いて単語重要度を算出し、それらを重要度分布として表現する。最後に、教師モデルと生徒モデルの重要度分布が近づくよう損失関数を設計し、先行研究で提案されている知識蒸留手法に当該損失項を導入することで、生徒モデルの学習を行う。本手法により、分類精度を先行研究と同程度に維持したまま、推論時に重要と判断される単語に関して教師モデルとの整合性を向上させることを狙う。

\section{本論文の構成}
本論文の構成は以下の通りである。2章では先行研究について述べる。3章では提案手法について詳細に説明する。4章では提案手法の評価実験について述べる。最後に5章では、本研究の手法についてのまとめと今後の課題について述べる。

\chapter{関連研究}
本章では本研究の関連研究について述べる。本章で述べる研究トピックはKnowledge Distillation（以下、知識蒸留）\cite{hinton2015distillingknowledgeneuralnetwork}である。知識蒸留は、性能が高くパラメータ数も大きいモデルを教師モデルとし、パラメータ数が小さいモデルへとタスクを解くための能力を転移されるための代表的な技術である。これまでに様々な知識蒸留手法が提案されてきており、教師モデルの出力層の知識を転移するものや、隠れ状態からの知識も転移させるもの、両方から知識を転移させるもの等、様々なパターンの知識蒸留のアプローチが提案されてきた。

また、分類モデルの知識蒸留における教師モデルとしてBERT\cite{devlin2019bertpretrainingdeepbidirectional}を用いられることが多い。BERTを教師モデルにして、知識蒸留されたモデルの代表的なものとしてTinyBERT\cite{jiao-etal-2020-tinybert}が存在する。

これらの言語モデルの推論根拠を提供する技術としてIntegrated Gradient\cite{sundararajan2017axiomaticattributiondeepnetworks}がある。Integrated Gradientsは画像処理分野で提案されたものではあるが本研究ではモデルが重要であると認識した単語を抽出するために利用する。

以下、\ref{kd-fundamental}節では、知識蒸留の原理的な設計について述べる。\ref{bert-model}節では、教師モデルとして用いるBERTについて紹介する。\ref{tinybert-model}節では、知識蒸留によって構築された代表的なモデルであるTinyBERTについて述べる。\ref{integrated-gradient}節ではモデルが推論する際の根拠を提示する技術としてIntegrated Gradientsを紹介する。最後に、\ref{methodlogy-characteristic}では、本研究の特色について述べる。

\section{知識蒸留}
\label{kd-fundamental}

知識蒸留（Knowledge Distillation）は、高性能である一方、パラメータ数が多い教師モデルから、
より小規模な生徒モデルへ特定のタスクを解く能力を転移させるための代表的な技術である。
一般に、教師モデルは対象タスクに対して十分に学習（ファインチューニング）されており、
生徒モデルは教師モデルの振る舞いを模倣することで効率的な学習を行う。

分類タスクでは、通常、あるドメインに対して数百から数万のラベル付きインスタンスが与えられる。
代表的な自然言語理解ベンチマークであるGLUE\cite{wang-etal-2018-glue}は、
複数の分類タスクから構成されるデータセット群であり、
知識蒸留の評価においても標準的に用いられる。
GLUEに含まれるMRPC（Microsoft Research Paraphrase Corpus）は、
2つの英文が意味的に等価であるかどうかを判定する二値分類タスクである。
各インスタンスにはラベル $y \in \{0,1\}$ が付与されており、
意味的に等価な場合を1、そうでない場合を0と定義する。

\begin{quote}
例\\
sentence1: Amrozi accused his brother, whom he called ``the witness'', of deliberately distorting his evidence.\\
sentence2: Referring to him as only ``the witness'', Amrozi accused his brother of deliberately distorting his evidence.\\
label: equivalent (1)
\end{quote}

通常の分類モデルの学習では、
生徒モデル $f_s$ が入力 $x$ に対して予測するクラス確率分布
$p_s(y \mid x)$ と、
正解ラベル $y$ との間のクロスエントロピー損失を最小化する。
この損失関数は、次式で定義される。
\begin{equation}
\mathcal{L}_{\mathrm{CE}}
= - \sum_{c} y_c \log p_s(c \mid x)
\label{eq:ce}
\end{equation}
ここで、$c$ はクラスを表し、
$y_c$ は正解クラスに対応するワンホット表現である。

知識蒸留では、
この正解ラベルに基づく学習に加えて、
同一の入力 $x$ に対する教師モデル $f_t$ の出力分布
$p_t(y \mid x)$ を、
生徒モデルが模倣することを目的とする。
教師モデルと生徒モデルの出力分布の乖離を測る指標として、
Kullback--Leibler（KL）ダイバージェンスが一般に用いられる。
このとき、知識蒸留損失は、次式で表される。
\begin{equation}
\mathcal{L}_{\mathrm{KD}}
= \mathrm{KL}\bigl(
p_t(\cdot \mid x)
\,\|\, 
p_s(\cdot \mid x)
\bigr)
\label{eq:kd}
\end{equation}
ここで、$\cdot$ はクラス全体を表し、
教師モデルが出力する確率分布全体を、
生徒モデルが近似することを意味する。

最終的な学習では、
正解ラベルに基づくクロスエントロピー損失と、
教師モデルの振る舞いを模倣する知識蒸留損失を組み合わせた、
次の損失関数を最小化する。
\begin{equation}
\mathcal{L}
= (1-\lambda)\mathcal{L}_{\mathrm{CE}}
\quad+\quad \lambda \mathcal{L}_{\mathrm{KD}}
\label{eq:total}
\end{equation}
ここで、$\lambda \in [0,1]$ はハイパーパラメータであり、
正解ラベルと教師モデルの知識のどちらを重視するかを制御する。

このように知識蒸留では、
データセットに明示的に含まれるラベル情報だけでなく、
教師モデルが獲得した予測分布という暗黙的な知識を
生徒モデルに転移することで、
高い性能を維持したままモデルの軽量化を実現することを目的としている。

\section{事前学習済み言語モデル}
\subsection{BERTの事前学習}
\label{bert-model}

BERT\cite{devlin-etal-2019-bert}は、Transformer\cite{vaswani2023attentionneed}を基盤とした双方向言語モデルであり、大規模コーパスを用いた事前学習によって汎用的な言語表現を獲得する。BERTの事前学習では、Masked Language Model（MLM）と Next Sentence Prediction（NSP）の2つのタスクが用いられる。

MLMでは、入力文中の一部の単語をマスクし、その単語を周辺文脈から予測することで単語表現を学習する。MLMの損失関数は以下のように定義される。
\begin{equation}
\mathcal{L}_{\mathrm{MLM}}
= - \sum_{i=1}^{N} \log p(t_i \mid T_i)
\label{eq:mlm}
\end{equation}
ここで、$t_i$ はマスクされた単語、$T_i$ は $Ft_i$ 以外の単語からなる入力文を表す。

次に、NSPは2つの文が元の文書中で連続しているか否かを判定するタスクであり、文間の関係性を学習することを目的としている。NSPの損失関数は次式で表される。
\begin{equation}
\mathcal{L}_{\mathrm{NSP}}
= - \left[
y \log P_{\mathrm{NSP}}
\; + \; (1 - y) \log (1 - P_{\mathrm{NSP}})
\right]
\label{eq:nsp}
\end{equation}
ここで、$y$ は文の連続性ラベルであり、2つの文が連続している場合は1、そうでない場合は0を取る。また、$P_{\mathrm{NSP}}$ は2文が連続しているとモデルが予測する確率を表す。
BERTの事前学習では、式(\ref{eq:mlm})および式(\ref{eq:nsp})で定義される損失を同時に最小化することで、単語レベルおよび文レベルの意味関係を考慮した言語表現を獲得する。

\subsection{教師モデルとしてのBERT}
本研究では教師モデルとして、BERTを用いる。BERTは事前学習によって大規模コーパスでの事前学習により汎用的な言語表現を獲得しており、このモデルに対して本研究で対象とするタスクでファインチューニングすることを行い、教師モデルとして採用する。
\section{知識蒸留によって構築されたモデル}
\subsection{TinyBERT}
\label{tinybert-model}
TinyBERT\cite{jiao-etal-2020-tinybert}は、BERTを教師モデルとして知識蒸留を適用することで構築された軽量な言語モデルである。TinyBERTは、教師モデルの出力層だけでなく、中間層の隠れ状態やAttention機構の重みも模倣することで、モデルサイズを大幅に削減しながら高い性能を維持することに成功している。

\subsubsection{データ拡張 (data augmentation)}
学習においてTinyBERTは下流タスクでの蒸留効果を高めるためにデータ拡張を併用することが知られている。具体的には、元のデータセットに対してランダムに単語を挿入・削除・置換することで多様な入力インスタンスを生成し、頑健性を向上させる。

\subsubsection{学習の流れ}
TinyBERTの蒸留は大きく2段階で行われる。

\paragraph{事前学習段階 (pre-training)}
事前学習段階では、BERTの事前学習で用いられたMasked Language Model（MLM）やNext Sentence Prediction（NSP）に加えて、中間層の隠れ状態やAttention重みの模倣を目的とした損失を導入する。これにより、生徒モデルは教師の内部表現を効率的に学習し、下流タスクへの基盤を構築する。

\paragraph{下流タスクのファインチューニング (downstream fine-tuning)}
下流タスクへの適応はさらに2段階（段階的蒸留）で実施される。

\begin{description}
	\setlength{\itemsep}{0.8\baselineskip}
	\item[第1段階 — 中間層蒸留 (intermediate layer distillation)]
	まず中間層の隠れ状態やAttention重みの模倣損失を導入し、生徒の内部表現を教師に近づけることで下流タスクへの適応準備を行う。

	\item[第2段階 — 出力層蒸留 (prediction layer distillation)]
	続いて教師の出力（ソフトラベル）とデータのハードラベルを併用して生徒の出力層を学習させ、教師モデルの推論能力を模倣させる。
\end{description}

TinyBERTはこの学習の流れを通じてBERTの知識を効率的に圧縮し、軽量ながら高性能なモデルを実現している。

\clearpage
\section{モデルの推論根拠の解釈に関する研究}
\label{model-explain}
\subsection{Integrated Gradient}
\label{integrated-gradient}

近年、深層学習モデルは画像認識や自然言語処理などの多くのタスクにおいて人間を凌駕する性能を達成している。しかし、その高い性能の代償として、モデル内部の計算プロセスは複雑化し、人間にとって理解困難な「ブラックボックス」となっていることで、モデルの推論における根拠となる特徴量を明示的に把握することが難しいという課題が存在する。

この要求に応えるための技術として、特徴量帰属法（Feature Attribution）が研究されている。特徴量帰属法は、モデルの予測結果に対する入力特徴量（例えば、画像の各ピクセルやテキストの各単語）の寄与度を定量的に算出する手法の総称である。寄与度が高い特徴量は、モデルが予測を行う上で重要な手がかりとして利用したことを示唆する。

Integrated Gradients は、ある入力 $x$ と、情報を持たない参照点であるベースライン $x'$ との間の経路に沿って勾配を積分することで定義される。
入力ベクトル $x \in \mathbb{R}^n$ における $i$ 番目の特徴量 $x_i$ の寄与度 $\mathrm{IG}_i(x)$ は以下の式で計算される。

\begin{equation}
\mathrm{IG}_i(x) = (x_i - x'_i) \int_{\alpha=0}^{1} \frac{\partial F(x' + \alpha(x - x'))}{\partial x_i} \, d\alpha
\end{equation}

ここで、$F$ は微分可能な機械学習モデル（例えばニューラルネットワークのソフトマックス出力値）、$x'$ はベースラインベクトルを表す。$\alpha$ は補間係数であり、$[0,1]$ の範囲で変化することで、ベースライン $x'$ から入力 $x$ への直線経路を辿る。この経路積分により、IGは以下の重要な公理的性質を満たす。

\paragraph{感度 (Sensitivity)}
モデルの予測に変化を与える全ての特徴量は、非ゼロの寄与度を持つべきであるという性質である。
従来の単純な勾配法（Simple Gradients）では、ReLUやSigmoidなどの活性化関数が飽和領域（勾配がほぼ0になる領域）に入ると、入力が変化しても勾配が消失し、寄与度が0と計算されてしまう問題があった。これを「勾配消失の問題」と呼ぶ。IGは、ベースラインから入力までの経路全体の勾配を積分するため、途中で勾配が存在すればその影響を捉えることができ、この感度の公理を満たすことができる。

\paragraph{完全性 (Completeness)}
各次元 $i$ における IG の総和は、常に入力 $x$ に対するモデル出力値とベースライン $x'$ に対するモデル出力値の差分に一致する。

\begin{equation}
\sum_{i=1}^n \mathrm{IG}_i(x) = F(x) - F(x')
\end{equation}

この性質は、算出された寄与度の合計がモデルの予測の変化分を過不足なく説明することを保証するものであり、説明の信頼性を担保する上で極めて重要である。

Integrated Gradients は入力変数が連続的に変化し、微分可能であることを前提とした手法である。しかし、自然言語処理で扱うテキストデータは離散的な「単語（トークン）」の列であり、入力値を微小に変化させて勾配を求めるという操作を直接行うことができない。そこでNLPタスクにおいては、単語を連続的なベクトル表現に変換する「埋め込み層（Embedding Layer）」の値を入力とみなしてIGを適用する方法が一般的に用いられる。

ある特定の単語 $w_t$ の寄与度を計算するためには、まずその単語に対応する埋め込みベクトルの各次元 $j$ についてのIG値 $\mathrm{IG}_{t,j}$ を計算する。

\begin{equation}
\mathrm{IG}_{t,j} = (e_{t,j} - b_{t,j}) \int_{\alpha=0}^{1} \frac{\partial F(B + \alpha(E - B))}{\partial e_{t,j}} \, d\alpha
\end{equation}

ここで、$E$ は入力文全体の埋め込み行列、$B$ はベースライン行列である。この式は、ベースラインの埋め込みから入力の埋め込みへと徐々に特徴を変化させながら、モデルの予測値の変化に対する勾配を累積していることを意味する。

最終的に、単語 $w_t$ としての重要度スコア $S_t$ を得るためには、埋め込みベクトルの各次元のIG値を集約する必要がある。集約方法としては、各次元の総和をとる方法や、L2ノルムをとる方法などが用いられる。

このようにして算出されたスコア $S_t$ が大きい単語ほど、モデルの予測判断に強く影響を与えた「重要な単語」であると解釈できる。

IGの計算には積分計算が必要であるが、コンピュータ上での実装においては、有限個のステップによるリーマン和近似が用いられる。積分区間 $[0,1]$ を $m$ 個のステップに分割した場合の近似式は以下のようになる。

\begin{equation}
\mathrm{IG}_i(x) \approx (x_i - x'_i) \frac{1}{m} \sum_{k=1}^{m} \frac{\partial F(x' + \frac{k}{m}(x - x'))}{\partial x_i}
\label{eq:ig}
\end{equation}

ここで、$m$ は近似のステップ数である。各ステップ $k$ においてモデルの逆伝搬計算を行う必要があるため、IGの計算コストはステップ数 $m$ に比例して増大する。
ステップ数が少なすぎると積分の近似誤差が大きくなり、積分誤差が大きくなる。一方でステップ数を多くすれば精度は向上するが、推論時間の数十倍から数百倍の計算時間を要することになる。
したがって、実用上は計算コストと精度のトレードオフを考慮し、近似誤差が計算時間の許容範囲内に収まるような適切なステップ数を設定する必要がある。

\section{本研究の特徴}
\label{methodlogy-characteristic}
従来の知識蒸留手法は、教師と生徒のモデル出力（ロジット）の確率分布を一致させることを目的としてきた。
これに対し、本研究は個々の単語がモデルの最終的な予測判断にどの程度貢献したのかを示すトークン重要度分布に焦点を当てる。Integrated Gradientsを用いてトークン重要度を定量化し、教師モデルと生徒モデルが同じ単語に注目して予測を行うよう学習させることで、「モデルの推論根拠」を明示的に転移させ、精度だけでなく信頼性と解釈可能性を備えた知識蒸留を実現する。

提案手法により、学習後の生徒モデルがどの単語に基づいて予測を行ったかが明らかになる。
教師と生徒のトークン重要度分布を合わせることで、両者の推論プロセスがより透明になり、推論プロセスが重視される領域に対して知識蒸留モデルの代替としての信頼性を向上させる。
\chapter{提案手法}
\section{概要}
本研究では、GLUEに含まれるタスクを対象し、生徒モデルを学習する際に教師モデルと同じ単語に注目するように推論するための損失項を提案する。

提案手法の概要を図\ref{fig:cola_jaccard}に示す。本提案手法は図中のStep 1とStep 2の2段階で構成される。

Step 1では、教師モデルと生徒モデルの双方に同一の学習データを入力し、推論結果に対してIntegrated Gradientsを適用することで各単語の重要度スコアを算出する。これにより、それぞれのモデルが入力文中のどの単語を推論の根拠として重要視しているかを表す重要度分布を作成する。

Step 2では、Step 1で得られた2つの重要度分布間の整合性を高めるための損失関数を定義する。この損失を最小化するように生徒モデルを学習させることで、生徒モデルは教師モデルと同様の単語に注目して推論を行うようにすることを狙う。
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{data/method_overview.png}
\caption{提案手法の概要}\label{fig:cola_jaccard}
\end{figure}

本研究で提案する損失項は、提案されてきた知識蒸留手法に追加で組み込むことができる。本研究における実験については、目的とするタスクでファインチューニングされていたBERTを教師モデルとして採用し、TinyBERTの学習手法に本提案手法を組み込むことで生徒モデルの学習を行う。
\section{重要度分布の作成}
\subsection{Integrated Gradientsによる単語重要度の算出}
本提案において、教師モデルと生徒モデルの重要度分布を作成する手順について述べる。具体的には学習の過程において学習データ1インスタンス毎に教師モデルと生徒モデルに対しIntegrated Gradientsを適用し、そのインスタンスのトークン長分個数の重要度スコアを算出する。GLUE/SST-2タスクにおけるインスタンスの例を以下に示す。SST-2タスクは、与えられた英文が肯定的か否定的かを判定する二値分類タスクである。括弧の中はモデルが予測すべきラベルに対応する。

\begin{quote}
例\\
sentence: seem weird and distanced\\
label: negative (0)
\end{quote}

このインスタンに対し、モデルは予測に際してトークンに分割をする。Integrated Gradientsを適用する際には、トークン毎に重要度スコアが算出される。上記の例に対し、モデルが以下のようなトークン分割を行ったとする。

\begin{quote}
トークン列: [CLS] seem weird and distance \#\#d [SEP]
\end{quote}

[CLS]と[SEP]は特殊トークンであり、トークナイザによって付加される。\#\#dはサブワードである。サブワードとは、単語をさらに細かく分割したものであり、BERTのトークナイザでは未知語に対してサブワード分割が行われる。上記の例では、distancedがdistanceと\#\#dに分割されている。分割された各トークンに対し、式\eqref{eq:ig}によるIntegrated Gradientsを適用することで、各トークン毎に重要度スコアが算出される。

Integrated Gradientsは埋め込みベクトルの各次元に対して計算される。最終的に、各トークンの重要度スコアは埋め込みベクトルの各次元のIG値を集約することで得られる。具体的には、BERTの埋め込みベクトルは768次元であるため、各トークンに対して768個のIG値が算出される。これらを集約することで、各トークンに対する1つの重要度スコアが得られる。

具体的に本研究で用いるSST-2でファインチューニングされたBERTモデルに対し、SST-2のあるインスタンスに対してIntegrated Gradientsを適用した際の各トークンの重要度スコアを示す。


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{data/method_ig_scores.png}
\caption{Integrated Gradientsによって算出された重要度スコア}\label{fig:method_ig_scores}
\end{figure}

以上のようにして、教師モデルと生徒モデルの双方に対し、学習データ1インスタンス毎にIntegrated Gradientsを適用し、各トークンの重要度スコアを算出する。算出されたIGスコアを集約する方法については、本研究ではL2ノルムを用いる。

学習データセット内のインスタンス $n$ における、トークン $t$ の重要度スコア $S_{n,t}$ は、以下の式で算出される。

\begin{equation}
S_{n,t} = \|\mathbf{IG}_{n,t}\|_2
\label{eq:l2norm}
\end{equation}

ここで、$\mathbf{IG}_{n,t}$ はインスタンス $n$ のトークン $t$ における埋め込みベクトルの各次元に対するIntegrated Gradientsの値をまとめたベクトルを表す。BERTでは埋め込みベクトルの次元数が $d=768$ であるため、$\mathbf{IG}_{n,t} \in \mathbb{R}^{768}$ となる。これにより、各トークンの重要度を単一のスカラー値として表現でき、トークン間の重要度比較が可能になる。本研究では、教師モデルおよび生徒モデルに対して、学習データセット内の各インスタンスごとにIntegrated Gradientsを適用し、各トークンの重要度スコアを算出する。

\subsection{重要度分布の作成}

前節で算出した各トークンの重要度スコアを用いて、教師モデルと生徒モデルの重要度分布を作成する。重要度スコア $S_{n,t}$ は各トークンがモデルの予測に与える影響の大きさを表す非負のスカラー値である。本研究では、この重要度スコアをそのまま用いるのではなく、確率分布に変換してから教師モデルと生徒モデルの比較を行う。

Integrated Gradientsによって算出される重要度スコアの値の範囲は、インスタンスごとに大きく異なる可能性がある。絶対値では大きく異なっても、各インスタンス内での相対的な重要性は類似している場合がある。しかし、重要度スコアの絶対値をそのまま用いると、この相対的な類似性を正しく捉えることができない。

この問題を解決するため、重要度スコアを確率分布に変換する。確率分布への変換では、各インスタンス内でのトークンの重要度スコアを正規化し、全トークンの確率の合計が1になるように調整する。これにより、重要度スコアの絶対的なスケールに依存せず、「各インスタンス内でどのトークンがどの程度重要か」という相対的な情報のみを表現できる。確率分布として表現することで、異なるスケールで算出された重要度スコアを統一的に扱い、教師モデルと生徒モデルの注目パターンを公平に比較できるようになる。

\clearpage
\textbf{(1) 温度スケーリング}

インスタンス $n$ のトークン $t$ における重要度スコア $S_{n,t}$ を温度パラメータ $\tau$ で除算する。

\begin{equation}
s_{n,t} = \frac{S_{n,t}}{\tau}
\label{eq:temperature_scaling}
\end{equation}

温度パラメータ $\tau$ は、重要度分布の鋭さを制御するハイパーパラメータである。$\tau > 1$ の場合、分布は平滑化され、より均一な確率分布に近づく。これにより、重要度が中程度のトークンにも適度な確率質量が割り当てられる。一方、$\tau < 1$ の場合、分布は鋭くなり、重要度が高いトークンへの確率集中が強調される。$\tau = 1$ の場合は、スケーリングを行わず、元の重要度スコアをそのまま用いることに相当する。本研究では、教師モデルと生徒モデルの重要度分布の比較において、いくつかの設定を試した結果、タスクによって温度を調整することが有効であると判断した。

\textbf{(2) ソフトマックス正規化}

次に、スケーリング後のスコアをソフトマックス関数で正規化し、確率分布に変換する。教師モデルの重要度分布 $p^T_{n,t}$ は以下の式で得られる：

\begin{equation}
p^T_{n,t} = \frac{\exp(s^T_{n,t})}{\sum_{t'=1}^{L} \exp(s^T_{n,t'})}
\label{eq:softmax_teacher}
\end{equation}

同様に、生徒モデルの重要度分布 $p^S_{n,t}$ は以下の式で得られる：

\begin{equation}
p^S_{n,t} = \frac{\exp(s^S_{n,t})}{\sum_{t'=1}^{L} \exp(s^S_{n,t'})}
\label{eq:softmax_student}
\end{equation}

ここで、$s^T_{n,t} = S^T_{n,t}/\tau$ は教師モデルの温度スケーリング後のスコア、$s^S_{n,t} = S^S_{n,t}/\tau$ は生徒モデルの温度スケーリング後のスコアを表す。$L$ はインスタンス $n$ のトークン長を表す。この正規化により、$\sum_{t=1}^{L} p^T_{n,t} = 1$ かつ $\sum_{t=1}^{L} p^S_{n,t} = 1$ を満たす確率分布が得られ、教師モデルと生徒モデルの重要度分布を比較できるようになる。

\clearpage
\section{重要度分布の最小化}
前節で作成した教師モデルと生徒モデルの重要度分布の類似度を測定し、その類似度を最大化するように生徒モデルを学習させる。本研究では、2つの分布の類似度を測る指標としてJaccard係数を用いる。Jaccard係数は、2つの集合の類似度を測る指標であり、2つの集合の共通部分の大きさを和集合の大きさで割った値として定義される：

\begin{equation}
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
\label{eq:jaccard}
\end{equation}

ここで、$A$ と $B$ はそれぞれ2つの集合を表す。Jaccard係数は $0$ から $1$ の値をとり、2つの集合が完全に一致する場合に $1$、共通部分が存在しない場合に $0$ となる。

しかしながら、従来のJaccard係数は離散的な集合に対して定義される指標である。本研究では、生徒モデルの学習において、損失関数を最小化するようにパラメータを更新する必要があり、そのためには損失関数が生徒モデルのパラメータに関して微分可能でなければならない。従来のJaccard係数は微分不可能であるため、勾配降下法によるパラメータ更新に用いることができない。

そこで、微分可能なSoft Jaccard Similarity\cite{jmlpaper}を用いる。Soft Jaccard Similarityは、従来のJaccard係数を連続的な確率分布に拡張したものであり、以下のように定義される。

\begin{equation}
J_{\text{soft}}(\mathbf{p}^T, \mathbf{p}^S) = \frac{\sum_{t=1}^{L} p^T_{n,t} \cdot p^S_{n,t}}{\sum_{t=1}^{L} (p^T_{n,t})^2 + \sum_{t=1}^{L} (p^S_{n,t})^2 - \sum_{t=1}^{L} p^T_{n,t} \cdot p^S_{n,t}}
\label{eq:soft_jaccard}
\end{equation}

ここで、$\mathbf{p}^T$ は教師モデルの重要度分布、$\mathbf{p}^S$ は生徒モデルの重要度分布を表す。分子は2つの分布の内積であり、従来のJaccard係数における積集合に相当する。分母は2つの分布の自乗和の和から内積を引いたものであり、和集合に相当する。Soft Jaccard Similarityは確率分布に対して連続的に定義されるため微分可能であり、損失関数として用いて勾配降下法により生徒モデルのパラメータを更新することが可能になる。

\section{本研究で提案する最終的な蒸留損失項}
本研究では以上の流れに従って、教師モデルと生徒モデルの重要度分布の類似度を最大化する損失項を提案する。最終的には以下の損失蒸留項として提案をまとめる。


\section{提案手法を用いた生徒モデルの学習}
\subsection{ベースライン}
\subsection{学習設定・タスク}
\subsection{損失関数の設計}





\chapter{評価}
\section{Jaccard係数による評価}
\subsection{実験条件}
\subsection{結果と考察}

\section{Rankingによる評価}
\subsection{実験条件}
\subsection{結果と考察}


\chapter{おわりに}
\section{本研究のまとめ}
\section{今後の課題}

% 参考文献
\renewcommand{\bibname}{参考文献}
% BibTeX を使うとき
\renewcommand{\em}{\it}% booktitle に下線を引かない
\bibliographystyle{plain}
\bibliography{reference}

% 付録
% \appendix
% \chapter{参考データ}

\end{document}
