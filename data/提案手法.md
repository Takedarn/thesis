# 提案手法: IG-Jaccard知識蒸留法の厳密な数式定義

## 概要
本手法は、**Integrated Gradients (IG)** を用いて抽出したトークン重要度分布に基づいて、教師モデルから学生モデルへ知識を蒸留する新しい手法である。従来の蒸留法とは異なり、最終的な出力ロジットではなく、各トークンの重要度の分布を一致させることで、より解釈可能で効果的な知識転移を実現する。

---

## 1. 記号の定義

### 1.1 基本記号
- $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$: 訓練データセット
  - $N$: サンプル数
  - $\mathbf{x}_i = [x_{i,1}, x_{i,2}, \ldots, x_{i,L}]$: 入力トークン列（長さ $L$）
  - $y_i \in \{1, 2, \ldots, C\}$: 正解ラベル（$C$はクラス数）

- $f_T$: 教師モデル（Teacher Model）
- $f_S$: 学生モデル（Student Model）

### 1.2 モデルの入出力
- $\mathbf{e}(\mathbf{x}) = [\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_L] \in \mathbb{R}^{L \times d}$: 埋め込みベクトル列
  - $\mathbf{e}_j \in \mathbb{R}^d$: トークン $x_j$ の埋め込みベクトル（$d$は埋め込み次元）
  
- $\mathbf{z} = f(\mathbf{e}(\mathbf{x})) \in \mathbb{R}^C$: モデルの出力ロジット

### 1.3 補助記号
- $\mathbf{m} \in \{0,1\}^L$: アテンションマスク
  - $m_j = 1$: 有効なトークン
  - $m_j = 0$: パディングトークン
  
- $\tau$: 温度パラメータ（Temperature）
- $M$: Integrated Gradientsの近似ステップ数

---

## 2. Integrated Gradients (IG) の厳密な定義

### 2.1 理論的基礎

Integrated Gradients は、ニューラルネットワークの予測に対する各入力特徴の寄与度を計算する手法である。入力 $\mathbf{e}$ に対する出力 $f(\mathbf{e})$ の第 $c$ クラスのスコアを $f^{(c)}(\mathbf{e})$ とする。

**定義 2.1 (Integrated Gradients の積分表現)**

ベースライン $\mathbf{e}'$（通常はゼロベクトル $\mathbf{0} \in \mathbb{R}^{L \times d}$）から入力 $\mathbf{e}$ への経路に沿った勾配の積分として定義される：

$$
\text{IG}(\mathbf{e})^{(c)} = \left(\mathbf{e} - \mathbf{e}'\right) \odot \int_{\alpha=0}^{1} \frac{\partial f^{(c)}\left(\mathbf{e}' + \alpha \cdot (\mathbf{e} - \mathbf{e}')\right)}{\partial \mathbf{e}} d\alpha
$$

ここで、
- $\odot$: 要素ごとの積（Hadamard積）
- $\alpha \in [0, 1]$: 補間パラメータ
- $\mathbf{e}' + \alpha \cdot (\mathbf{e} - \mathbf{e}')$: ベースラインから入力への直線経路上の点

**式の意味**: 各入力特徴が出力に与える影響を、ベースラインからの変化量と経路上の勾配の積として定量化する。

### 2.2 リーマン和による離散近似

積分を数値的に計算するため、リーマン和で近似する：

**定義 2.2 (IG の離散近似)**

$$
\text{IG}(\mathbf{e})^{(c)} \approx \left(\mathbf{e} - \mathbf{e}'\right) \odot \frac{1}{M} \sum_{m=1}^{M} \frac{\partial f^{(c)}\left(\mathbf{e}' + \frac{m}{M} \cdot (\mathbf{e} - \mathbf{e}')\right)}{\partial \mathbf{e}}
$$

ここで、
- $M$: 近似ステップ数（通常 $M=10$ 程度）
- $\frac{m}{M}$: 第 $m$ ステップの補間係数 $\alpha_m$

**計算手順**:
1. $m = 1, 2, \ldots, M$ に対して、補間点を計算:
   $$
   \mathbf{e}_m = \mathbf{e}' + \frac{m}{M} \cdot (\mathbf{e} - \mathbf{e}')
   $$

2. 各補間点での勾配を計算:
   $$
   \mathbf{g}_m = \frac{\partial f^{(c)}(\mathbf{e}_m)}{\partial \mathbf{e}}
   $$

3. 勾配の平均を計算:
   $$
   \bar{\mathbf{g}} = \frac{1}{M} \sum_{m=1}^{M} \mathbf{g}_m
   $$

4. IGを計算:
   $$
   \text{IG}(\mathbf{e})^{(c)} = (\mathbf{e} - \mathbf{e}') \odot \bar{\mathbf{g}}
   $$

### 2.3 トークンレベルの重要度スコア

各トークンの重要度スコアは、そのトークンに対応する埋め込みベクトルの寄与度の $L_2$ ノルムとして定義する：

**定義 2.3 (トークン重要度スコア)**

トークン $j$ の重要度 $I_j^{(c)}$ は以下で定義される：

$$
I_j^{(c)} = \left\| \text{IG}(\mathbf{e})_j^{(c)} \right\|_2 = \sqrt{\sum_{k=1}^{d} \left(\text{IG}(\mathbf{e})_{j,k}^{(c)}\right)^2}
$$

ここで、
- $\text{IG}(\mathbf{e})_j^{(c)} \in \mathbb{R}^d$: トークン $j$ の埋め込み次元に対するIG
- $\|\cdot\|_2$: $L_2$ ノルム（ユークリッドノルム）

**式の意味**: 埋め込み空間全体での寄与度を1つのスカラー値に集約することで、トークンごとの重要度を直接比較可能にする。

### 2.4 バッチ処理での実装

バッチサイズ $B$ の場合、各サンプル $i \in \{1, \ldots, B\}$ に対して独立にIGを計算する：

$$
\mathbf{I}^{(c)} = \left[I_{1,1}^{(c)}, \ldots, I_{1,L}^{(c)}, I_{2,1}^{(c)}, \ldots, I_{B,L}^{(c)}\right] \in \mathbb{R}^{B \times L}
$$

ここで、$I_{i,j}^{(c)}$ はバッチ内の $i$ 番目のサンプルの $j$ 番目のトークンの重要度である。

---

## 3. Soft Jaccard Similarity の定義

### 3.1 従来のJaccard係数

集合 $A, B$ に対する古典的なJaccard係数は以下で定義される：

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}
$$

ここで、
- $|A \cap B|$: 積集合の要素数
- $|A \cup B|$: 和集合の要素数

**性質**:
- $J(A, B) \in [0, 1]$
- $J(A, B) = 1 \Leftrightarrow A = B$
- $J(A, B) = 0 \Leftrightarrow A \cap B = \emptyset$

### 3.2 Soft Jaccard Similarity への拡張

確率分布やソフトな重み付けに対応するため、Jaccard係数を連続的に拡張する。

**定義 3.1 (Soft Jaccard Similarity)**

2つの確率分布 $\mathbf{p} = [p_1, \ldots, p_L]$ と $\mathbf{q} = [q_1, \ldots, q_L]$ に対して、Soft Jaccard Similarity を以下で定義する：

$$
J_{\text{soft}}(\mathbf{p}, \mathbf{q}) = \frac{\sum_{j=1}^{L} p_j \cdot q_j}{\sum_{j=1}^{L} p_j^2 + \sum_{j=1}^{L} q_j^2 - \sum_{j=1}^{L} p_j \cdot q_j}
$$

**各項の意味**:
1. **分子** $\sum_{j=1}^{L} p_j \cdot q_j$: 
   - 2つの分布の内積（類似度の指標）
   - 集合の積集合 $|A \cap B|$ のソフト版

2. **分母** $\sum_{j=1}^{L} p_j^2 + \sum_{j=1}^{L} q_j^2 - \sum_{j=1}^{L} p_j \cdot q_j$:
   - 2つの分布の自乗和の和から内積を引いたもの
   - 集合の和集合 $|A \cup B|$ のソフト版

**性質の検証**:
- $J_{\text{soft}}(\mathbf{p}, \mathbf{q}) \in [0, 1]$
- $\mathbf{p} = \mathbf{q}$ のとき: 
  $$
  J_{\text{soft}}(\mathbf{p}, \mathbf{p}) = \frac{\sum p_j^2}{2\sum p_j^2 - \sum p_j^2} = \frac{\sum p_j^2}{\sum p_j^2} = 1
  $$
- $\mathbf{p} \perp \mathbf{q}$（直交）のとき:
  $$
  J_{\text{soft}}(\mathbf{p}, \mathbf{q}) = \frac{0}{\sum p_j^2 + \sum q_j^2} = 0
  $$

### 3.3 安定化技術

数値的安定性を確保するため、ゼロ除算を防ぐ微小値 $\epsilon$ を導入する：

$$
J_{\text{soft}}(\mathbf{p}, \mathbf{q}) = \frac{\sum_{j=1}^{L} p_j \cdot q_j}{\sum_{j=1}^{L} p_j^2 + \sum_{j=1}^{L} q_j^2 - \sum_{j=1}^{L} p_j \cdot q_j + \epsilon}
$$

ここで、$\epsilon = 10^{-8}$ 程度の値を用いる。

---

## 4. IG-Jaccard 蒸留損失の完全な定義

### 4.1 トークン重要度の確率分布化

教師モデルと学生モデルのトークン重要度を確率分布に変換する。

**ステップ4.1.1: 教師モデルのトークン重要度計算**

入力 $\mathbf{x}$ とラベル $y$ に対して、教師モデルのトークン重要度を計算：

$$
\mathbf{I}_T = [I_{T,1}, I_{T,2}, \ldots, I_{T,L}]
$$

ここで、$I_{T,j} = I_j^{(y)}$ は定義2.3で計算される。

**ステップ4.1.2: 温度スケーリング**

重要度スコアを温度パラメータ $\tau$ でスケーリング：

$$
s_{T,j} = \frac{I_{T,j}}{\tau}
$$

**温度パラメータの役割**:
- $\tau > 1$: 分布を平滑化（より均一に）
- $\tau < 1$: 分布を鋭くする（ピークを強調）
- $\tau = 1$: スケーリングなし

**ステップ4.1.3: パディングマスキング**

パディングトークンの影響を除去するため、マスキングを適用：

$$
\tilde{s}_{T,j} = \begin{cases}
s_{T,j} & \text{if } m_j = 1 \\
-\infty & \text{if } m_j = 0
\end{cases}
$$

実装上は、$-\infty$ の代わりに $-10^9$ などの大きな負の値を用いる。

**ステップ4.1.4: ソフトマックス正規化**

マスキング後のスコアをソフトマックスで確率分布に変換：

$$
p_{T,j} = \frac{\exp(\tilde{s}_{T,j})}{\sum_{k=1}^{L} \exp(\tilde{s}_{T,k})}
$$

ここで、$\mathbf{p}_T = [p_{T,1}, \ldots, p_{T,L}]$ は教師モデルのトークン重要度分布である。

**性質**:
- $\sum_{j=1}^{L} p_{T,j} = 1$
- $p_{T,j} \geq 0 \ \forall j$
- パディング位置では $p_{T,j} \approx 0$

**ステップ4.1.5: 学生モデルでの同様の処理**

学生モデルに対しても同様に計算し、学生モデルのトークン重要度分布 $\mathbf{p}_S = [p_{S,1}, \ldots, p_{S,L}]$ を得る。

### 4.2 IG-Jaccard損失関数

**定義 4.1 (IG-Jaccard損失関数)**

単一サンプル $(\mathbf{x}, y)$ に対するIG-Jaccard損失は以下で定義される：

$$
\mathcal{L}_{\text{IG-Jaccard}}(\mathbf{x}, y) = 1 - J_{\text{soft}}(\mathbf{p}_T, \mathbf{p}_S)
$$

展開すると：

$$
\mathcal{L}_{\text{IG-Jaccard}}(\mathbf{x}, y) = 1 - \frac{\sum_{j=1}^{L} p_{T,j} \cdot p_{S,j}}{\sum_{j=1}^{L} p_{T,j}^2 + \sum_{j=1}^{L} p_{S,j}^2 - \sum_{j=1}^{L} p_{T,j} \cdot p_{S,j} + \epsilon}
$$

**損失関数の解釈**:
- $J_{\text{soft}}(\mathbf{p}_T, \mathbf{p}_S) = 1$ （完全一致）のとき: $\mathcal{L} = 0$（損失最小）
- $J_{\text{soft}}(\mathbf{p}_T, \mathbf{p}_S) = 0$ （完全不一致）のとき: $\mathcal{L} = 1$（損失最大）

### 4.3 バッチ平均損失

バッチ $\mathcal{B} = \{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_B, y_B)\}$ に対する平均損失：

$$
\mathcal{L}_{\text{batch}} = \frac{1}{B} \sum_{i=1}^{B} \mathcal{L}_{\text{IG-Jaccard}}(\mathbf{x}_i, y_i)
$$

この損失を最小化するように学生モデルのパラメータを更新する。

---

## 5. 勾配計算と最適化

### 5.1 二重バックプロパゲーション

IG-Jaccard損失の特徴は、**二重のバックプロパゲーション**を必要とすることである。

**第1段階: IG計算時の勾配**
- 各補間点 $\mathbf{e}_m$ での出力 $f^{(y)}(\mathbf{e}_m)$ から埋め込み $\mathbf{e}_m$ への勾配を計算

**第2段階: 損失からモデルパラメータへの勾配**
- IG-Jaccard損失 $\mathcal{L}$ から学生モデルのパラメータ $\theta_S$ への勾配を計算

**数式表現**:

$$
\frac{\partial \mathcal{L}}{\partial \theta_S} = \frac{\partial \mathcal{L}}{\partial \mathbf{p}_S} \cdot \frac{\partial \mathbf{p}_S}{\partial \mathbf{I}_S} \cdot \frac{\partial \mathbf{I}_S}{\partial \theta_S}
$$

ここで、$\frac{\partial \mathbf{I}_S}{\partial \theta_S}$ の計算にIG内部での勾配計算が含まれるため、計算グラフを保持する必要がある（`create_graph=True`）。

### 5.2 計算効率の考慮

**教師モデルの扱い**:
- 教師モデルのパラメータ $\theta_T$ は固定（更新しない）
- 教師のIGは定数として扱う: $\mathbf{I}_T = \mathbf{I}_T.detach()$
- 計算グラフを保持しないため、メモリ効率が向上

**学生モデルの扱い**:
- 学生モデルのパラメータ $\theta_S$ を更新
- 学生のIG計算時に `create_graph=True` を指定し、二重バックプロパゲーションを可能にする

---

## 6. アルゴリズム全体の流れ

### アルゴリズム 6.1: IG-Jaccard知識蒸留

**入力**:
- 訓練データ $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$
- 教師モデル $f_T$（事前学習済み）
- 学生モデル $f_S$（初期化済み）
- ハイパーパラメータ: $\tau, M, \epsilon$, 学習率 $\eta$, エポック数 $E$

**出力**:
- 蒸留された学生モデル $f_S^*$

**手順**:

```
1: for epoch = 1 to E do
2:     for each batch B ⊂ D do
3:         // 教師のトークン重要度計算（勾配不要）
4:         I_T ← ComputeIG(f_T, B, steps=M, create_graph=False)
5:         I_T ← I_T.detach()  // 定数化
6:         
7:         // 教師の確率分布化
8:         s_T ← I_T / τ
9:         s_T ← MaskPadding(s_T, attention_mask)
10:        p_T ← Softmax(s_T)
11:        
12:        // 学生のトークン重要度計算（勾配保持）
13:        I_S ← ComputeIG(f_S, B, steps=M, create_graph=True)
14:        
15:        // 学生の確率分布化
16:        s_S ← I_S / τ
17:        s_S ← MaskPadding(s_S, attention_mask)
18:        p_S ← Softmax(s_S)
19:        
20:        // Soft Jaccard Similarity 計算
21:        product ← sum(p_T ⊙ p_S)
22:        sum_sq_T ← sum(p_T²)
23:        sum_sq_S ← sum(p_S²)
24:        J_soft ← product / (sum_sq_T + sum_sq_S - product + ε)
25:        
26:        // 損失計算
27:        L ← mean(1 - J_soft)  // バッチ平均
28:        
29:        // バックプロパゲーションと更新
30:        L.backward()
31:        Update θ_S with learning rate η
32:        Zero gradients
33:    end for
34: end for
35: return f_S
```

**補助関数**:

```
Function ComputeIG(model, batch, steps, create_graph):
    Input: model, batch=(x, y, mask), steps M, create_graph flag
    Output: token importance I ∈ R^(B×L)
    
    1: e ← model.embeddings(x)  // 入力埋め込み
    2: e' ← zeros_like(e)       // ベースライン（ゼロ）
    3: total_grads ← 0
    4: 
    5: for m = 1 to M do
    6:     α ← m / M
    7:     e_interp ← e' + α(e - e')  // 補間
    8:     logits ← model(inputs_embeds=e_interp, mask=mask)
    9:     score ← logits[range(B), y].sum()  // ターゲットクラスのスコア
    10:    grads ← ∂score/∂e_interp
    11:    total_grads ← total_grads + grads
    12: end for
    13: 
    14: avg_grads ← total_grads / M
    15: IG ← e ⊙ avg_grads  // Hadamard積
    16: I ← ||IG||_2 along embedding dim  // L2ノルムでトークンごとに集約
    17: return I
```

---

## 7. 数学的性質と理論的保証

### 7.1 勾配の存在性

**補題 7.1**: ソフトマックス関数とSoft Jaccard Similarityはいずれも微分可能であり、$\mathcal{L}_{\text{IG-Jaccard}}$ は学生モデルのパラメータ $\theta_S$ に関して微分可能である。

**証明のスケッチ**:
1. ソフトマックス関数は全域で微分可能
2. Soft Jaccard Similarityは分母が非ゼロ（$\epsilon$ により保証）の範囲で微分可能
3. 合成関数の微分可能性により、$\mathcal{L}$ は $\theta_S$ に関して微分可能 □

### 7.2 損失の有界性

**補題 7.2**: IG-Jaccard損失は有界である：$\mathcal{L}_{\text{IG-Jaccard}} \in [0, 1]$

**証明**:
- Soft Jaccard Similarityの定義より、$J_{\text{soft}} \in [0, 1]$
- したがって、$\mathcal{L} = 1 - J_{\text{soft}} \in [0, 1]$ □

### 7.3 最適解の特性

**定理 7.1**: $\mathcal{L}_{\text{IG-Jaccard}} = 0$ の必要十分条件は $\mathbf{p}_T = \mathbf{p}_S$ である。

**証明**:
- (⇐) $\mathbf{p}_T = \mathbf{p}_S$ のとき、3.2節の性質より $J_{\text{soft}}(\mathbf{p}_T, \mathbf{p}_T) = 1$、よって $\mathcal{L} = 0$
- (⇒) $\mathcal{L} = 0$ のとき、$J_{\text{soft}}(\mathbf{p}_T, \mathbf{p}_S) = 1$。
  
  Cauchy-Schwarzの不等式より、
  $$
  \left(\sum p_{T,j} p_{S,j}\right)^2 \leq \sum p_{T,j}^2 \cdot \sum p_{S,j}^2
  $$
  
  等号成立条件は $\mathbf{p}_T$ と $\mathbf{p}_S$ が比例する場合、すなわち $\mathbf{p}_S = c\mathbf{p}_T$。
  
  両者が確率分布（総和=1）であることから、$c=1$、すなわち $\mathbf{p}_T = \mathbf{p}_S$ □

---

## 8. ハイパーパラメータの選択指針

### 8.1 温度パラメータ $\tau$
- **推奨値**: $\tau \in [1, 5]$
- **効果**: 
  - 小さい値: 重要なトークンを強調（シャープな分布）
  - 大きい値: 全トークンを均等に扱う（平滑な分布）

### 8.2 IGステップ数 $M$
- **推奨値**: $M \in [10, 50]$
- **効果**:
  - 大きい値: 積分の近似精度向上、計算コスト増
  - 小さい値: 計算効率向上、近似誤差増

### 8.3 安定化パラメータ $\epsilon$
- **推奨値**: $\epsilon = 10^{-8}$
- **効果**: ゼロ除算防止、数値安定性確保

---

## 9. 計算複雑度解析

### 9.1 時間計算量

単一バッチ（サイズ $B$）に対する計算量：

1. **IG計算** (教師・学生各1回):
   - フォワードパス: $O(M \cdot T_{\text{forward}})$
   - バックワードパス: $O(M \cdot T_{\text{backward}})$
   - 合計: $O(M \cdot (T_{\text{forward}} + T_{\text{backward}}))$

2. **ソフトマックス正規化**: $O(B \cdot L)$

3. **Soft Jaccard計算**: $O(B \cdot L)$

**全体**: $O(M \cdot T_{\text{model}} + B \cdot L)$

ここで、$T_{\text{model}}$ はモデルの1回のフォワード+バックワード計算時間。

### 9.2 空間計算量

- **勾配保持**: 学生モデルのIG計算で $M$ 個の中間状態を保持
- **メモリ使用量**: $O(M \cdot B \cdot L \cdot d)$

**メモリ最適化**: 勾配チェックポインティングにより $O(\sqrt{M} \cdot B \cdot L \cdot d)$ に削減可能。

---

## 10. 従来手法との比較

### 10.1 KL divergence ベース蒸留との違い

**従来のKD損失**:
$$
\mathcal{L}_{\text{KD}} = \text{KL}\left(\text{softmax}(\mathbf{z}_T/\tau) \| \text{softmax}(\mathbf{z}_S/\tau)\right)
$$

- 出力ロジット $\mathbf{z}$ の分布を一致
- **ブラックボックス**: 内部のトークン重要度は考慮しない

**提案手法のIG-Jaccard損失**:
$$
\mathcal{L}_{\text{IG-Jaccard}} = 1 - J_{\text{soft}}(\mathbf{p}_T^{\text{token}}, \mathbf{p}_S^{\text{token}})
$$

- トークンレベルの重要度分布 $\mathbf{p}^{\text{token}}$ を一致
- **ホワイトボックス**: 各トークンの寄与度を明示的に学習

### 10.2 Attention-based 蒸留との違い

**Attention蒸留**:
$$
\mathcal{L}_{\text{Attn}} = \text{MSE}(\mathbf{A}_T, \mathbf{A}_S)
$$

- アテンション行列 $\mathbf{A} \in \mathbb{R}^{L \times L}$ を一致
- **トークン間関係**: トークン同士の関係性を学習

**提案手法**:
- トークンの**絶対的重要度**を学習
- 出力への寄与度に基づく解釈可能性

---

## 11. 実装上の注意点

### 11.1 数値安定性

**Softmaxのオーバーフロー対策**:
$$
p_j = \frac{\exp(s_j - \max_k s_k)}{\sum_{k} \exp(s_k - \max_k s_k)}
$$

**Soft Jaccardの分母ゼロ対策**:
- $\epsilon = 10^{-8}$ を追加
- 勾配クリッピング: $\|\nabla_{\theta_S} \mathcal{L}\| \leq \text{clip\_value}$

### 11.2 計算グラフの管理

**教師モデル**:
```python
with torch.no_grad():  # パラメータ勾配不要
    I_T = compute_ig(teacher, batch, create_graph=False)
I_T = I_T.detach()  # 計算グラフから切り離す
```

**学生モデル**:
```python
I_S = compute_ig(student, batch, create_graph=True)  # 二重バックプロパゲーション用
```

### 11.3 メモリ最適化

- **勾配累積**: バッチサイズを小さく、累積ステップ数を増やす
- **混合精度学習**: FP16で計算、FP32で更新
- **勾配チェックポインティング**: 中間活性化を再計算

---

## 12. まとめ

本手法は、以下の3つの主要コンポーネントから構成される：

1. **Integrated Gradients (定義2.1, 2.2)**:
   $$
   \text{IG}(\mathbf{e})^{(c)} = (\mathbf{e} - \mathbf{e}') \odot \frac{1}{M} \sum_{m=1}^{M} \frac{\partial f^{(c)}(\mathbf{e}_m)}{\partial \mathbf{e}}
   $$
   各トークンの出力への寄与度を定量化

2. **Soft Jaccard Similarity (定義3.1)**:
   $$
   J_{\text{soft}}(\mathbf{p}, \mathbf{q}) = \frac{\sum p_j q_j}{\sum p_j^2 + \sum q_j^2 - \sum p_j q_j + \epsilon}
   $$
   確率分布間の類似度を測定

3. **IG-Jaccard損失 (定義4.1)**:
   $$
   \mathcal{L}_{\text{IG-Jaccard}} = 1 - J_{\text{soft}}(\mathbf{p}_T^{\text{token}}, \mathbf{p}_S^{\text{token}})
   $$
   教師と学生のトークン重要度分布を一致させる

この3つを組み合わせることで、**解釈可能**かつ**効果的**な知識蒸留を実現する。
---

## 参考文献

1. Sundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic attribution for deep networks. *ICML*.
2. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. *NIPS Workshop*.
3. Jaccard, P. (1901). Distribution de la flore alpine dans le bassin des Dranses et dans quelques régions voisines. *Bulletin de la Société Vaudoise des Sciences Naturelles*.
